<!DOCTYPE HTML>
<html lang="en">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Hongjie Li</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWFQX2P75H"></script>
    <!-- <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    </script> -->

    <meta name="author" content="Hongjie Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="msvalidate.01" content="1D6EAEB9C6558C0BB977413398D67E91" />
    
    <link rel="stylesheet" type="text/css" href="static/css/stylesheet.css">
    <script src="static/js/script.js"></script>
    <link rel="icon" href="static/images/icon.png">
  </head>

  <body>
    <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Hongjie Li</name>
                </p>
                <p>
                  I am a 4th-year undergraduate student in <a href="https://eecs.pku.edu.cn/" target="_blank">School of EECS</a>, <a href="https://www.pku.edu.cn" target="_blank">Peking University</a>. I'm also a member of the  <a href="https://zhi-class.ai/" target="_blank">PKU Zhi Class</a>.
                  I am currently a student researcher in <a href="https://pku.ai" target="_blank">Cognitive Reasoning (CoRe) Lab</a> at PKU-IAI, advised by <a href="https://yzhu.io" target="_blank">Prof. Yixin Zhu</a>.
                  I spent a wonderful summer in <a href="https://svl.stanford.edu/" target="_blank">Stanford Vision and Learning Lab (SVL)</a> at <a href="https://www.stanford.edu/" target="_blank">Stanford University</a> as a research intern, advised by <a href="https://jiajunwu.com/" target="_blank">Prof. Jiajun Wu</a>.
                </p>
                <p>
                  My research interests lie in 3D computer vision and graphics, including human-object/scene interaction (HOI/HSI), 3D scene understanding, and generative modeling.
                </p>
                <p style="text-align:center">
                  <a href="mailto:lihongjie@stu.pku.edu.cn">Email</a>
                  &nbsp/&nbsp<a href="https://scholar.google.com/citations?user=elzAX5YAAAAJ&hl=en" target="_blank">Google Scholar</a>
                  &nbsp/&nbsp<a href="https://github.com/awfuact" target="_blank">Github</a>
                  &nbsp/&nbsp<a href="static/files/CV_Hongjie_Li_main.pdf" target="_blank">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:25%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="static/images/Personal/hli_0.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:15px;padding-top:0;padding-bottom:0;width:100%;vertical-align:middle">
                <heading>Publications</heading>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/cvpr25_motionrefit/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Dynamic Motion Blending for Versatile Motion Editing</papertitle>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <strong>Hongjie Li</strong>*,
                <a href="https://pku.ai/author/ziye-yuan/" target="_blank">Ziye Yuan</a>*,
                <a href="https://mileret.github.io/" target="_blank">Zimo He</a>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/cvpr25_motionrefit/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2503.20724" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/emptybulebox1/motionRefit/" target="_blank">[Code]</a>
                &nbsp
                <a href="https://drive.google.com/file/d/1LiNgkRZ-Kmv5rKI3BOaHVCudhrMtE5hx/view?usp=sharing" target="_blank">[Data]</a>
                &nbsp
                <a href="https://awfuact.github.io/motionrefit/" target="_blank">[Project]</a>
                &nbsp
                <a href="https://huggingface.co/spaces/Yzy00518/motionReFit" target="_blank">[Hugging Face]</a>
                <br>
                <abs>
                  Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets (original motion, edited motion, and instruction), which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models (LLMs). Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing. Ablation studies further verify that MotionCutMix significantly improves the model's generalizability while maintaining training convergence.
                </abs>
                <p>
                  tl;dr: Our text-guided motion editor combines dynamic body-part blending augmentation with an auto-regressive diffusion model to enable diverse motion editing without extensive pre-collected training data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/arxiv24_zerohsi/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</papertitle>
                <br>
                <strong>Hongjie Li</strong>*,
                <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>*,
                <a href="https://lijiaman.github.io/" target="_blank">Jiaman Li</a>,
                <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/arxiv24_zerohsi/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2412.18600" target="_blank">[arXiv]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Code]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Data]</a> -->
                &nbsp
                <a href="https://awfuact.github.io/zerohsi/" target="_blank">[Project]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                <br>
                <abs>
                  Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
                </abs>
                <p>
                  tl;dr: We leverage video generation models and differentiable rendering to synthesize zero-shot human-scene interactions without requiring motion capture data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/siggraph-asia24_lingo/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Autonomous Character-Scene Interaction Synthesis from Text Instruction</papertitle>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://mileret.github.io/" target="_blank">Zimo He</a>*,
                <a>Zi Wang</a>,
                <strong>Hongjie Li</strong>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2024
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/siggraph-asia24_lingo/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2410.03187" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/mileret/lingo-release" target="_blank">[Code]</a>
                &nbsp
                <a href="https://drive.google.com/file/d/1RadpLt-woPvsIGk9yDW7yX7br3sRO-zi/view" target="_blank">[Data]</a>
                &nbsp
                <a href="https://lingomotions.com/" target="_blank">[Project]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                &nbsp
                <a href="https://techxplore.com/news/2024-11-framework-motions-human-characters-3d.html" target="_blank">[TechXplore]</a>
                <br>
                <abs>
                  Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and Human-Object Interaction (HOI), presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured (MoCap) dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.
                </abs>
                <p>
                  tl;dr: We present a framework that synthesizes multi-stage scene-aware human interaction motions from just text instructions and goal locations.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/cvpr24_trumans/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Scaling Up Dynamic Human-Scene Interaction Modeling</papertitle>
                <!-- <a href="#" class="teaser-link" style="display:none;margin-left:10px">[teaser]</a> -->
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://zhiyuan-zhang0206.github.io/" target="_blank">Zhiyuan Zhang</a>*,
                <strong>Hongjie Li</strong>,
                <a href="https://shirleymaxx.github.io/" target="_blank">Xiaoxuan Ma</a>,
                <a href="https://silvester.wang/" target="_blank">Zan Wang</a>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
                <br>
                <em>CVPR</em>, 2024
                <font color="red"><b>(Highlight)</b></font>
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/cvpr24_trumans/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2403.08629" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/jnnan/trumans_utils" target="_blank">[Code]</a>
                &nbsp
                <a href="https://docs.google.com/forms/d/e/1FAIpQLSdF62BQ9TQTSTW0HzyNeRPhlzmREL5T8hUGn-484W1I3eVihQ/viewform" target="_blank">[Data]</a>
                &nbsp
                <a href="https://jnnan.github.io/trumans/" target="_blank">[Project]</a>
                &nbsp
                <a href="https://huggingface.co/spaces/jnnan/trumans" target="_blank">[Hugging Face]</a>
                <br>
                <abs>
                  Confronting the challenges of data scarcity and advanced motion synthesis in HSI modeling, we introduce the TRUMANS (Tracking Human Actions in Scenes) dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates Human-Scene Interaction (HSI) sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.
                </abs>
                <p>
                  tl;dr: We introduce the largest motion-captured HSI dataset, enabling our diffusion-based autoregressive model to generate realistic human-scene interactions with across various 3D environments.
                </p>
              </td>
            </tr>
          </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/svl.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Stanford Vision and Learning Lab (SVL)</b>, Stanford University, USA
                <br> Jun 2024 - Aug 2024
                <br>
                <br> <b>Research Intern</b>
                <br> Advisor: <a href="https://jiajunwu.com/" target="_blank">Prof. Jiajun Wu</a>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/core_lab.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Cognitive Reasoning (CoRe) Lab</b>, Institute for Artificial Intelligence, Peking University, China
                <br> Jan 2023 - Present
                <br>
                <br> <b>Student Researcher</b>
                <br> Advisor: <a href="http://yzhu.io" target="_blank">Prof. Yixin Zhu</a>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/pku.jpeg", width="100px"></td>
              <td width="90%" valign="center">
                <b>Peking University</b>, China
                <br> Aug 2021 - Present
                <br>
                <br> <b>Undergraduate Student</b>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Updated on April 28, 2025.
                  <br>
                  Template from <a href="https://jonbarron.info/" style="text-align:right;font-size:small;" target="_blank">Jon Barron</a>. 
                  Icons by <a href="https://www.flaticon.com/authors/freepik" style="text-align:right;font-size:small;" target="_blank">Freepik</a>.
                  Technical support from <a style="font-size:small;" href="https://yuyangli.com">Yuyang Li</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>

</html>
