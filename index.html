<!DOCTYPE HTML>
<html lang="en">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Hongjie Li's Homepage</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWFQX2P75H"></script>
    <!-- <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    </script> -->

    <meta name="author" content="Hongjie Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="msvalidate.01" content="1D6EAEB9C6558C0BB977413398D67E91" />
    
    <link rel="stylesheet" type="text/css" href="static/css/stylesheet.css">
    <script src="static/js/script.js"></script>
    <link rel="icon" href="static/images/icon.png">
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&display=swap" rel="stylesheet">
  </head>

  <body>
    <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Hongjie Li</name>
                </p>
                <p>
                  I am a senior in <a href="https://zhi-class.ai/" target="_blank">Zhi Class</a> at <a href="https://eecs.pku.edu.cn/" target="_blank">School of EECS</a>, <a href="https://www.pku.edu.cn" target="_blank">Peking University</a>.
                  I am currently a student researcher in <a href="https://pku.ai" target="_blank">Cognitive Reasoning Lab</a> at Peking University, advised by <a href="https://yzhu.io" target="_blank">Prof. Yixin Zhu</a>.
                  I am also a student researcher in General Vision Lab, <a href="https://www.bigai.ai/about/" target="_blank">BIGAI</a>, and I'm grateful to be advised by <a href="https://siyuanhuang.com" target="_blank">Dr. Siyuan Huang</a>.
                  I spent two wonderful summers in <a href="https://svl.stanford.edu/" target="_blank">Stanford Vision and Learning Lab</a> at <a href="https://www.stanford.edu/" target="_blank">Stanford University</a>, advised by <a href="https://jiajunwu.com/" target="_blank">Prof. Jiajun Wu</a>.
                </p>
                <p>
                  My research interests lie in 3D computer vision and graphics, including human-object/scene interaction (HOI/HSI), humanoid robot learning, 3D scene understanding, and visual generative modeling. My long-term goal is to build machines that perceive and reason about the physical world through interaction, ultimately developing algorithms that enable embodied agents like digital humans and robots to autonomously interact with their surroundings as humans do.
                </p>
                <p style="text-align:center">
                  <a href="mailto:lihongjie@stu.pku.edu.cn">Email</a>
                  &nbsp/&nbsp<a href="https://scholar.google.com/citations?user=elzAX5YAAAAJ&hl=en" target="_blank">Google Scholar</a>
                  &nbsp/&nbsp<a href="https://github.com/awfuact" target="_blank">Github</a>
                  &nbsp/&nbsp<a href="static/files/CV_Hongjie_Li_main.pdf" target="_blank">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:25%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="static/images/Personal/Hongjie.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:15px;padding-top:0;padding-bottom:0;width:100%;vertical-align:middle">
                <heading>Recent News</heading>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:5px;padding-top:0;padding-bottom:15px;width:100%;vertical-align:middle">
                  <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2026: One paper is accepted to CVPR 2026.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2025: One paper is accepted to 3DV 2026.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2025: I'll be joining Stanford Vision and Learning Lab as a summer research intern.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2025: One paper is accepted to CVPR 2025.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2024: One paper is (conditionally) accepted to SIGGRAPH Asia 2024.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: I'll be joining Stanford Vision and Learning Lab as a summer research intern.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2024: One paper is accepted to CVPR 2024.</li>
                  </ul>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:15px;padding-top:0;padding-bottom:0;width:100%;vertical-align:middle">
                <heading>Publications</heading>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/cvpr26_anylift/teaser.png', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>AnyLift: Scaling Motion Reconstruction from Internet Videos via 2D Diffusion</papertitle>
                <br>
                <strong>Hongjie Li</strong>*,
                <a href="https://heng14.github.io/" target="_blank">Heng Yu</a>*,
                <a href="https://lijiaman.github.io/" target="_blank">Jiaman Li</a>,
                <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
                <a href="https://stanford.edu/~eadeli/" target="_blank">Ehsan Adeli</a>,
                <a href="https://stanford-tml.github.io/main/" target="_blank">Karen Liu</a>,
                <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
                <br>
                <em>CVPR</em>, 2026
                <br>
                <a href="#" target="_blank">[Abs]</a>
                <!-- &nbsp
                <a href="" target="_blank">[PDF]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[arXiv]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Code]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Data]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Project]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                <br>
                <abs>
                  Reconstructing 3D human motion and human-object interactions (HOI) from Internet videos is a fundamental step toward building large-scale datasets of human behavior. Existing methods struggle to recover globally consistent 3D motion under dynamic cameras, especially for motion types underrepresented in current motion-capture datasets, and face additional difficulty recovering coherent human-object interactions in 3D. We introduce a two-stage framework leveraging 2D diffusion that reconstructs 3D human motion and HOI from Internet videos. In the first stage, we synthesize multi-view 2D motion data for each domain, leveraging 2D keypoints extracted from Internet videos to incorporate human motions that rarely appear in existing MoCap datasets. In the second stage, a camera-conditioned multi-view 2D motion diffusion model is trained on these domain-specific synthetic data to recover 3D human motion and 3D HOI in the world space. We demonstrate the effectiveness of our method on Internet videos featuring challenging motions such as gymnastics, as well as in-the-wild HOI videos, and show that it outperforms prior work in producing realistic human motion and human-object interaction.
                </abs>
                <p>
                  tl;dr: We propose a two-stage 2D diffusion framework that reconstructs globally consistent 3D human motion and human-object interactions from Internet videos with dynamic cameras.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/3dv26_zerohsi/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</papertitle>
                <br>
                <strong>Hongjie Li</strong>*,
                <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>*,
                <a href="https://lijiaman.github.io/" target="_blank">Jiaman Li</a>,
                <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
                <br>
                <em>3DV</em>, 2026
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/3dv26_zerohsi/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2412.18600" target="_blank">[arXiv]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Code]</a> -->
                <!-- &nbsp
                <a href="" target="_blank">[Data]</a> -->
                &nbsp
                <a href="https://awfuact.github.io/zerohsi/" target="_blank">[Project]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                <br>
                <abs>
                  Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
                </abs>
                <p>
                  tl;dr: We leverage video generation models and differentiable rendering to synthesize zero-shot human-scene interactions without requiring motion capture data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/arxiv25_uniact/teaser.png', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots</papertitle>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://mileret.github.io/" target="_blank">Zimo He</a>*,
                <a href="https://pku.ai/author/lexi-pang/" target="_blank">Lexi Pang</a>,
                <a href="https://pku.ai/author/wanhe-yu/" target="_blank">Wanhe Yu</a>,
                <a href="https://pku.ai/author/yunhao-li/" target="_blank">Yunhao Li</a>,
                <strong>Hongjie Li</strong>,
                <a href="https://jiemingcui.github.io/" target="_blank">Jieming Cui</a>,
                <a>Yuhan Li</a>,
                <a href="https://cfcs.pku.edu.cn/people/faculty/yizhouwang/index.htm" target="_blank">Yizhou Wang</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/arxiv25_uniact/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2512.24321" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/jnnan/uniact-code" target="_blank">[Code]</a>
                &nbsp
                <a href="https://jnnan.github.io/uniact/#dataset-section" target="_blank">[Data]</a>
                &nbsp
                <a href="https://jnnan.github.io/uniact/#" target="_blank">[Project]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                <br>
                <abs>
                  A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions---such as language, music, and trajectories---into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned Multimodal Large Language Model (MLLM) with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via Finite Scalar Quantization (FSQ), UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate  UniAct on UA-Net, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.
                </abs>
                <p>
                  tl;dr: We introduce a unified framework that enables humanoid robots to execute multimodal instructions by generating motion tokens through a shared representation and achieving real-time whole-body control.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/cvpr25_motionrefit/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Dynamic Motion Blending for Versatile Motion Editing</papertitle>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <strong>Hongjie Li</strong>*,
                <a href="https://pku.ai/author/ziye-yuan/" target="_blank">Ziye Yuan</a>*,
                <a href="https://mileret.github.io/" target="_blank">Zimo He</a>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/cvpr25_motionrefit/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2503.20724" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/emptybulebox1/motionRefit/" target="_blank">[Code]</a>
                &nbsp
                <a href="https://drive.google.com/file/d/1LiNgkRZ-Kmv5rKI3BOaHVCudhrMtE5hx/view?usp=sharing" target="_blank">[Data]</a>
                &nbsp
                <a href="https://awfuact.github.io/motionrefit/" target="_blank">[Project]</a>
                &nbsp
                <a href="https://huggingface.co/spaces/Yzy00518/motionReFit" target="_blank">[Hugging Face]</a>
                &nbsp
                <a href="https://techxplore.com/news/2025-04-dynamic-generate-realistic-human-motions.html" target="_blank">[TechXplore]</a>
                <br>
                <abs>
                  Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets (original motion, edited motion, and instruction), which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models (LLMs). Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing. Ablation studies further verify that MotionCutMix significantly improves the model's generalizability while maintaining training convergence.
                </abs>
                <p>
                  tl;dr: Our text-guided motion editor combines dynamic body-part blending augmentation with an auto-regressive diffusion model to enable diverse motion editing without extensive pre-collected training data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/siggraph-asia24_lingo/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Autonomous Character-Scene Interaction Synthesis from Text Instruction</papertitle>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://mileret.github.io/" target="_blank">Zimo He</a>*,
                <a>Zi Wang</a>,
                <strong>Hongjie Li</strong>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2024
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/siggraph-asia24_lingo/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2410.03187" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/mileret/lingo-release" target="_blank">[Code]</a>
                &nbsp
                <a href="https://drive.google.com/file/d/1RadpLt-woPvsIGk9yDW7yX7br3sRO-zi/view" target="_blank">[Data]</a>
                &nbsp
                <a href="https://lingomotions.com/" target="_blank">[Project]</a>
                <!-- &nbsp
                <a href="" target="_blank">[Hugging Face]</a> -->
                &nbsp
                <a href="https://techxplore.com/news/2024-11-framework-motions-human-characters-3d.html" target="_blank">[TechXplore]</a>
                <br>
                <abs>
                  Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and Human-Object Interaction (HOI), presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured (MoCap) dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.
                </abs>
                <p>
                  tl;dr: We present a framework that synthesizes multi-stage scene-aware human interaction motions from just text instructions and goal locations.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:18px;vertical-align:middle"><img src='static/publications/cvpr24_trumans/teaser.jpg', width="300px"></td>
              <td style="padding:15px;width:80%;vertical-align:middle">
                <papertitle>Scaling Up Dynamic Human-Scene Interaction Modeling</papertitle>
                <!-- <a href="#" class="teaser-link" style="display:none;margin-left:10px">[teaser]</a> -->
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://zhiyuan-zhang0206.github.io/" target="_blank">Zhiyuan Zhang</a>*,
                <strong>Hongjie Li</strong>,
                <a href="https://shirleymaxx.github.io/" target="_blank">Xiaoxuan Ma</a>,
                <a href="https://silvester.wang/" target="_blank">Zan Wang</a>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
                <br>
                <em>CVPR</em>, 2024
                <font color="red"><b>(Highlight)</b></font>
                <br>
                <a href="#" target="_blank">[Abs]</a>
                &nbsp
                <a href="static/publications/cvpr24_trumans/paper.pdf" target="_blank">[PDF]</a>
                &nbsp
                <a href="https://arxiv.org/abs/2403.08629" target="_blank">[arXiv]</a>
                &nbsp
                <a href="https://github.com/jnnan/trumans_utils" target="_blank">[Code]</a>
                &nbsp
                <a href="https://docs.google.com/forms/d/e/1FAIpQLSdF62BQ9TQTSTW0HzyNeRPhlzmREL5T8hUGn-484W1I3eVihQ/viewform" target="_blank">[Data]</a>
                &nbsp
                <a href="https://jnnan.github.io/trumans/" target="_blank">[Project]</a>
                &nbsp
                <a href="https://huggingface.co/spaces/jnnan/trumans" target="_blank">[Hugging Face]</a>
                <br>
                <abs>
                  Confronting the challenges of data scarcity and advanced motion synthesis in HSI modeling, we introduce the TRUMANS (Tracking Human Actions in Scenes) dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates Human-Scene Interaction (HSI) sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.
                </abs>
                <p>
                  tl;dr: We introduce the largest motion-captured HSI dataset, enabling our diffusion-based autoregressive model to generate realistic human-scene interactions across various 3D environments.
                </p>
              </td>
            </tr>
          </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:15px;width:100%;padding-top:0;padding-bottom:0;vertical-align:middle">
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/svl.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Stanford Vision and Learning Lab (SVL)</b>, Stanford University, USA
                <br> Jun 2025 - Dec 2025
                <br> Jun 2024 - Dec 2024
                <br>
                <br> <b>Research Intern</b>
                <br> Advisor: <a href="https://jiajunwu.com/" target="_blank">Prof. Jiajun Wu</a>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/bigai.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Beijing Institute for General Artificial Intelligence (BIGAI)</b>, China
                <br> Sept 2023 - Present
                <br>
                <br> <b>Student Researcher</b>
                <br> Advisor: <a href="https://siyuanhuang.com" target="_blank">Dr. Siyuan Huang</a>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/core_lab.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Cognitive Reasoning (CoRe) Lab</b>, Peking University, China
                <br> Jan 2023 - Present
                <br>
                <br> <b>Student Researcher</b>
                <br> Advisor: <a href="http://yzhu.io" target="_blank">Prof. Yixin Zhu</a>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="static/images/Institution/pku.jpeg", width="100px"></td>
              <td width="90%" valign="center">
                <b>Peking University</b>, China
                <br> Aug 2021 - Present
                <br>
                <br> <b>Undergraduate Student</b>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last updated: Feb. 2026
                  <br>
                  Template from <a href="https://jonbarron.info/" style="text-align:right;font-size:small;" target="_blank">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>

</html>
